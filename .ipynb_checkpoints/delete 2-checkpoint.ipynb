{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.11.0\n",
      "argon2-cffi @ file:///C:/ci/argon2-cffi_1596826740294/work\n",
      "astunparse==1.6.3\n",
      "async-generator==1.10\n",
      "attrs @ file:///tmp/build/80754af9/attrs_1604765588209/work\n",
      "backcall==0.2.0\n",
      "bleach @ file:///tmp/build/80754af9/bleach_1611690509631/work\n",
      "cached-property==1.5.2\n",
      "cachetools==4.2.0\n",
      "certifi==2020.12.5\n",
      "cffi @ file:///C:/ci/cffi_1606255201193/work\n",
      "chardet==4.0.0\n",
      "colorama @ file:///tmp/build/80754af9/colorama_1607707115595/work\n",
      "cycler==0.10.0\n",
      "decorator==4.4.2\n",
      "defusedxml==0.6.0\n",
      "entrypoints==0.3\n",
      "findspark==1.4.2\n",
      "flatbuffers==1.12\n",
      "gast==0.3.3\n",
      "google-auth==1.24.0\n",
      "google-auth-oauthlib==0.4.2\n",
      "google-pasta==0.2.0\n",
      "grpcio==1.32.0\n",
      "h5py==2.10.0\n",
      "idna==2.10\n",
      "importlib-metadata==3.3.0\n",
      "ipykernel @ file:///C:/ci/ipykernel_1596206775157/work/dist/ipykernel-5.3.4-py3-none-any.whl\n",
      "ipython @ file:///C:/ci/ipython_1593446240034/work\n",
      "ipython-genutils @ file:///tmp/build/80754af9/ipython_genutils_1606773439826/work\n",
      "jedi @ file:///C:/ci/jedi_1611341129952/work\n",
      "Jinja2 @ file:///home/linux1/recipes/ci/jinja2_1610990516718/work\n",
      "jsonschema @ file:///tmp/build/80754af9/jsonschema_1602607155483/work\n",
      "jupyter-client @ file:///tmp/build/80754af9/jupyter_client_1601311786391/work\n",
      "jupyter-core @ file:///C:/ci/jupyter_core_1606149071529/work\n",
      "jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work\n",
      "Keras==2.4.3\n",
      "Keras-Preprocessing==1.1.2\n",
      "kiwisolver==1.3.1\n",
      "Markdown==3.3.3\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib==3.3.3\n",
      "mistune==0.8.4\n",
      "mkl-fft==1.2.0\n",
      "mkl-random==1.1.1\n",
      "mkl-service==2.3.0\n",
      "nbclient @ file:///tmp/build/80754af9/nbclient_1602783176460/work\n",
      "nbconvert @ file:///C:/ci/nbconvert_1601896775581/work\n",
      "nbformat @ file:///tmp/build/80754af9/nbformat_1610738111109/work\n",
      "nest-asyncio @ file:///tmp/build/80754af9/nest-asyncio_1606153767164/work\n",
      "notebook @ file:///C:/ci/notebook_1611348388909/work\n",
      "numpy @ file:///C:/ci/numpy_and_numpy_base_1603480701039/work\n",
      "oauthlib==3.1.0\n",
      "opt-einsum==3.3.0\n",
      "packaging @ file:///tmp/build/80754af9/packaging_1607971725249/work\n",
      "pandas @ file:///C:/ci/pandas_1602088210462/work\n",
      "pandocfilters @ file:///C:/ci/pandocfilters_1605120535071/work\n",
      "parso @ file:///tmp/build/80754af9/parso_1607623074025/work\n",
      "pickleshare @ file:///tmp/build/80754af9/pickleshare_1606932040724/work\n",
      "Pillow==8.0.1\n",
      "prometheus-client @ file:///tmp/build/80754af9/prometheus_client_1606344362066/work\n",
      "prompt-toolkit @ file:///tmp/build/80754af9/prompt-toolkit_1602688806899/work\n",
      "protobuf==3.14.0\n",
      "py4j==0.10.9\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycparser @ file:///tmp/build/80754af9/pycparser_1594388511720/work\n",
      "Pygments @ file:///tmp/build/80754af9/pygments_1610565767015/work\n",
      "pyparsing @ file:///home/linux1/recipes/ci/pyparsing_1610983426697/work\n",
      "pyrsistent @ file:///C:/ci/pyrsistent_1600141799440/work\n",
      "pyspark==3.0.1\n",
      "python-dateutil==2.8.1\n",
      "pytz @ file:///tmp/build/80754af9/pytz_1606604771399/work\n",
      "pywin32==227\n",
      "pywinpty==0.5.7\n",
      "PyYAML==5.3.1\n",
      "pyzmq==20.0.0\n",
      "requests==2.25.1\n",
      "requests-oauthlib==1.3.0\n",
      "rsa==4.6\n",
      "scipy==1.5.4\n",
      "seaborn==0.11.1\n",
      "Send2Trash @ file:///tmp/build/80754af9/send2trash_1607525499227/work\n",
      "six @ file:///C:/ci/six_1605187303045/work\n",
      "tensorboard==2.4.0\n",
      "tensorboard-plugin-wit==1.7.0\n",
      "tensorflow==2.4.0\n",
      "tensorflow-estimator==2.4.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.9.2\n",
      "testpath==0.4.4\n",
      "tornado @ file:///C:/ci/tornado_1606942379977/work\n",
      "traitlets==4.3.3\n",
      "typing-extensions==3.7.4.3\n",
      "urllib3==1.26.2\n",
      "wcwidth @ file:///tmp/build/80754af9/wcwidth_1593447189090/work\n",
      "webencodings==0.5.1\n",
      "Werkzeug==1.0.1\n",
      "wincertstore==0.2\n",
      "wrapt==1.12.1\n",
      "xlrd @ file:///tmp/build/80754af9/xlrd_1608072521494/work\n",
      "zipp @ file:///tmp/build/80754af9/zipp_1604001098328/work\n"
     ]
    }
   ],
   "source": [
    "! pip freeze\n",
    "#checking the required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start a pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "# import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.appName('spark').getOrCreate()\n",
    "# create a new pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-VHQRRVE5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x25c38639ac8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking=spark.read.json('data/roomBookings.json',header=True,inferSchema=True)\n",
    "rooms=spark.read.json('data/rooms.json',header=True,inferSchema=True)\n",
    "rooms_schedule=spark.read.json('data/roomSchedule.json',header=True,inferSchema=True)\n",
    "#loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).json(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).json(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-3f7fb219c6d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrooms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\dsenv\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    438\u001b[0m         \"\"\"\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dsenv\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dsenv\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dsenv\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).json(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).json(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().;"
     ]
    }
   ],
   "source": [
    "rooms.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_corrupt_record: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
